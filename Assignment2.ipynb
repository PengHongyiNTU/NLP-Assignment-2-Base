{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE7455 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: PENG HONGYI <br>\n",
    "Matric No: G2105029E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quetion One (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Named Entity Recognition__ (NER) as an important task in NLP, attemps to classify predefined entities in a sentence. In our assignment, we use _eng.train_ for training, _eng.testa_ for validation and _eng.testb_ for testing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence in the dataset is presented below, where the first columns is the input word and the last column is the output tag. The dataset contains four different types of predefined entities: PERSON, LOCATION, ORGANIZATION, and MISC. As shown in the fourth column, the fourth column contains the groudtruth entity name and the BIO tag, seperated by '-'.\n",
    "\n",
    "| Word    |     |      | Tag    |\n",
    "|---------|-----|------|--------|\n",
    "| EU      | NNP | I-NP | I-ORG  |\n",
    "| rejects | VBZ | I-VP | O      |\n",
    "| German  | JJ  | I-NP | I-MISC |\n",
    "| call    | NN  | I-NP | O      |\n",
    "| to      | TO  | I-VP | O      |\n",
    "| boycott | VB  | I-VP | O      |\n",
    "| British | JJ  | I-NP | I-MISC |\n",
    "| lamb    | NN  | I-NP | O      |\n",
    "| .       | .   | O    | O      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question one (ii)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 preprocessing steps in the code provided:\n",
    "* Replacing all the digit with 0.\n",
    "* Convert BIO tagging to BIOES tagging.\n",
    "* Generate words mapping.\n",
    "* Generate tag mapping.\n",
    "* Generate chracter mapping.\n",
    "Mappings here are dictionaries that assign an integer id to every unique word, character and tag. After the proprocessing step, we found: \n",
    "\n",
    "> Found 17493 unique words (203621 in total) \n",
    "Found 75 unique characters \n",
    "Found 19 unique named entity tags\n",
    "\n",
    "The preprocessed dataset is stored in _data/mapping.pkl_. To save time, we will directly processed data throughout this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_to_id', 'tag_to_id', 'char_to_id', 'parameters', 'word_embeds']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "import pickle\n",
    "with open('data/mapping.pkl', 'rb') as f:\n",
    "    mapping = pickle.load(f)\n",
    "list(mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = mapping['word_to_id']\n",
    "tag_to_id = mapping['tag_to_id']\n",
    "char_to_id = mapping['char_to_id']\n",
    "# We use our own parameters\n",
    "# parameters = mapping['parameters']\n",
    "word_embeds = mapping['word_embeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"./data/eng.train\" #Path to train file\n",
    "parameters['dev'] = \"./data/eng.testa\" #Path to test file\n",
    "parameters['test'] = \"./data/eng.testb\" #Path to dev file\n",
    "parameters['tag_scheme'] = \"BIOES\" #BIO or BIOES\n",
    "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
    "parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
    "parameters['char_dim'] = 30 #Char embedding dimension\n",
    "parameters['word_dim'] = 100 #Token embedding dimension\n",
    "parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n",
    "parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
    "parameters['embedding_path'] = \"./data/glove.6B.100d.txt\" #Location of pretrained embeddings\n",
    "parameters['all_emb'] = 1 #Load all embeddings\n",
    "parameters['crf'] =1 #Use CRF (0 to disable)\n",
    "parameters['dropout'] = 0.5 #Droupout on the input (0 = no dropout)\n",
    "parameters['epoch'] =  50 #Number of epochs to run\"\n",
    "parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
    "parameters['name'] = \"self-trained-model\" # Model name\n",
    "parameters['gradient_clip']=5.0\n",
    "parameters['char_mode']=\"CNN\"\n",
    "models_path = \"./models/\" #path to saved models\n",
    "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
    "use_gpu = parameters['use_gpu']\n",
    "parameters['reload'] = \"./models/pre-trained-model\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import load_sentences\n",
    "from TagConversion import update_tag_scheme\n",
    "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
    "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
    "val_sentences = load_sentences(parameters['dev'], parameters['zeros'])\n",
    "update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(val_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(test_sentences, parameters['tag_scheme'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041 / 3250 / 3453 sentences in train / val / test.\n"
     ]
    }
   ],
   "source": [
    "from Utils import prepare_dataset\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "val_data = prepare_dataset(\n",
    "    val_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "print(\"{} / {} / {} sentences in train / val / test.\".format(len(train_data), len(val_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lianran/phy/NLP-Assignment-2-Base/Init.py:8: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(input_embedding, -bias, bias)\n",
      "/home/lianran/phy/NLP-Assignment-2-Base/Init.py:44: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
      "/home/lianran/phy/NLP-Assignment-2-Base/Init.py:49: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
      "/home/lianran/phy/NLP-Assignment-2-Base/Init.py:57: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
      "/home/lianran/phy/NLP-Assignment-2-Base/Init.py:60: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
      "/home/lianran/phy/NLP-Assignment-2-Base/Init.py:16: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(input_linear.weight, -bias, bias)\n"
     ]
    }
   ],
   "source": [
    "from BaseModel import BiLSTM_CRF\n",
    "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf=parameters['crf'],\n",
    "                   char_mode=parameters['char_mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base model contain 2284255 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters(model)\n",
    "print(f'The base model contain {num_parameters} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model reloaded : ./models/pre-trained-model\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(parameters['reload']))\n",
    "print(\"model reloaded :\", parameters['reload'])\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from Helper import get_chunks\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(model, datas):\n",
    "    prediction = []\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "    for data in tqdm(datas, total=len(datas)):\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "        chars2 = data['chars']\n",
    "        if parameters['char_mode'] == 'LSTM':\n",
    "            chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "            d = {}\n",
    "            for i, ci in enumerate(chars2):\n",
    "                for j, cj in enumerate(chars2_sorted):\n",
    "                    if ci == cj and not j in d and not i in d.values():\n",
    "                        d[j] = i\n",
    "                        continue\n",
    "            chars2_length = [len(c) for c in chars2_sorted]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros(\n",
    "                (len(chars2_sorted), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2_sorted):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        if parameters['char_mode'] == 'CNN':\n",
    "            d = {}\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros(\n",
    "                (len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        if use_gpu:\n",
    "            val, out = model(\n",
    "                dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "        else:\n",
    "            val, out = model(dwords, chars2_mask, chars2_length, d)\n",
    "        predicted_id = out\n",
    "        lab_chunks = set(get_chunks(ground_truth_id, tag_to_id))\n",
    "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
    "                                         tag_to_id))\n",
    "\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "\n",
    "    p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    F = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3453/3453 [03:46<00:00, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Test F1-Score: 0.8401554170055119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_score = evaluate(model, test_data)\n",
    "print(f'Original Model Test F1-Score: {f_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the trained model provided at https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu and evaluate it performance on the test set. The test f1-score is:\n",
    "> trained model: __0.84__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question one (iii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either an CNN and an LSTM can be used to perform character-level encoding\n",
    "In the provided code, the CNN is declared as \n",
    "```\n",
    "char_cnn = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
    "```\n",
    "Whereas, the LSTM is defined as\n",
    "```\n",
    "char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
    "                init_lstm(self.char_lstm)\n",
    "```\n",
    "No matter what characte-level encoder is used, the extracted character-level representation will be contactenated with higher-level word embeddings and be fed into a Bidirectional lstm. However, for different encoder, the input dimension for the higher-level LSTM are different. If CNN is used, the input dimension is word_embedding_dim + out_channeles. If LSTM is used, the input dimension is word_embedding_dim + char_lstm_dim * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question one (iV)\n",
    "As mentioned in the previous section, word embeddings, contactenated with the characte level embedding, are fed into an LSTM.\n",
    "In this section, we will replace the LSTM with CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's check the output dimension of the word-level lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(125, 200, bidirectional=True)\n",
      "torch.Size([8, 1, 400])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn(8, 1, 125).cuda()\n",
    "    lstm = model.lstm\n",
    "    print(lstm)\n",
    "    lstm_y, _ = lstm(x)\n",
    "    print(lstm_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the input of the word-level lstm has shape $(L, N, H_{emb})$ where $L$ is the length of sequences, in our case the length of the input sentences, $N$ is the number of samples in the mini-batch. The provided code adopts single-batch training. Thus, $N=1$. $H_{emb}$ is the dimension of input embedding. In our case, $H_{emb} = H_{word} + H_{char} = 100 + 25 = 125$. The output of word-level lstm has shape $(L, N, 2*H_{ltsm})$ since our lstm is bi-directional.(we set $H_{lstm}=200$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nn.Conv1d()__ takes a $(N, C_{in}, L_{in})$ tensor as input. To make sure that the output dimension is the same after replacing lstm with cnn, we need to transpose the input tensor, and set the output channel $C_{out}$ = 400. Moreover, we set the kernal size equals to 3, if no padding added, the output of cnn will be $(N, C_{out}, L_{in}-3+1)$. In order to not to change $L$, the padding is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([1, 125, 8])\n",
      "Output: torch.Size([1, 400, 8])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "KERNAL_SIZE = 3\n",
    "with torch.no_grad():\n",
    "    cnn = nn.Conv1d(in_channels=125, out_channels=400, kernel_size=3, padding=1).cuda()\n",
    "    x = x.squeeze()\n",
    "    x_t = x.transpose(0, 1)\n",
    "    x_t = x_t.unsqueeze(0)\n",
    "    print('Input: ', x_t.shape)\n",
    "    y = cnn(x_t)\n",
    "    print('Output:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "lstm_y= lstm_y.view(8, -1)\n",
    "y = y.squeeze_().t()\n",
    "print(lstm_y.shape == y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace the lstm layer, we define a new class that inherit the provided model and modify the __get_lstm_features()__ function. Although, it should be called __get_cnn_features()__ now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCNN_WordEncoderModel(BiLSTM_CRF):\n",
    "    def __init__(self, char_lstm_dim=25, *args, **kwargs):\n",
    "        self.char_lstm_dim = char_lstm_dim\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Init word-level cnn\n",
    "        self.word_cnn = nn.Conv1d(\n",
    "            in_channels=self.embedding_dim + self.out_channels,\n",
    "            out_channels=2*self.hidden_dim,\n",
    "            kernel_size=KERNAL_SIZE,\n",
    "            padding=1    \n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
    "        if self.char_mode == 'LSTM':\n",
    "            chars_embeds = self.char_embeds(chars2).transpose(0, 1)\n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                chars_embeds, chars2_length)\n",
    "            lstm_out, _ = self.char_lstm(packed)\n",
    "            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "                lstm_out)\n",
    "            outputs = outputs.transpose(0, 1)\n",
    "            chars_embeds_temp = Variable(torch.FloatTensor(\n",
    "                torch.zeros((outputs.size(0), outputs.size(2)))))\n",
    "            if self.use_gpu:\n",
    "                chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "            for i, index in enumerate(output_lengths):\n",
    "                chars_embeds_temp[i] = torch.cat(\n",
    "                    (outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))\n",
    "            chars_embeds = chars_embeds_temp.clone()\n",
    "            for i in range(chars_embeds.size(0)):\n",
    "                chars_embeds[d[i]] = chars_embeds_temp[i]\n",
    "        if self.char_mode == 'CNN':\n",
    "            chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
    "            chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
    "            chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
    "                                                    kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
    "        embeds = self.word_embeds(sentence)\n",
    "        embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "        # embeds = embeds.unsqueeze(1)\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Orignal Code for LSTM Features \n",
    "        #################################################################\n",
    "        # Word lstm\n",
    "            # lstm_out, _ = self.lstm(embeds)\n",
    "            # print('lstm-out', lstm_out.shape)\n",
    "\n",
    "        # Reshaping the outputs from the lstm layer\n",
    "            # lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "            # print('lstm_out view change', embeds.shape)\n",
    "\n",
    "        # Dropout on the lstm output\n",
    "            # lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Linear layer converts the ouput vectors to tag space\n",
    "        #################################################################\n",
    "        # Our Code for CNN Features \n",
    "        # The embed size now is (L, 125)\n",
    "        embeds = embeds.transpose(0, 1)\n",
    "        # embede size: (125, L)\n",
    "        embeds = embeds.unsqueeze(0)\n",
    "        # embdes size: (1, 125, L)\n",
    "        embeds = self.word_cnn(embeds)\n",
    "        # embedes size: (1, 400, L)\n",
    "        cnn_out = embeds.squeeze_().t()\n",
    "        # embedes size: (400, L) => (L, 400)\n",
    "        lstm_feats = self.hidden2tag(cnn_out)\n",
    "        return lstm_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordCNNmodel = OneCNN_WordEncoderModel(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf=parameters['crf'],\n",
    "                   char_mode=parameters['char_mode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the total number of parameters in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2434655"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(WordCNNmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we declare the training function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data, save_dir = 'models/cnn-word.pt'):\n",
    "    model.train(True)\n",
    "    model.cuda()\n",
    "    losses = []\n",
    "    best_valid = 0.\n",
    "    stop_count = 5\n",
    "    valid_scores = []\n",
    "    learning_rate = 0.015\n",
    "    momentum = 0.9\n",
    "    decay_rate = 0.05\n",
    "    gradient_clip = parameters['gradient_clip']\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        loss = 0\n",
    "        print(f'Epoch {epoch} training starts')\n",
    "        p_bar = tqdm(enumerate(np.random.permutation(len(train_data))), total=len(train_data))\n",
    "        for i, index in p_bar:\n",
    "            data = train_data[index]\n",
    "            model.zero_grad()\n",
    "            sentence_in = data['words']\n",
    "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "            tags = data['tags']\n",
    "            chars2 = data['chars']\n",
    "            if parameters['char_mode'] == 'LSTM':\n",
    "                chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "                d = {}\n",
    "                for i, ci in enumerate(chars2):\n",
    "                    for j, cj in enumerate(chars2_sorted):\n",
    "                        if ci == cj and not j in d and not i in d.values():\n",
    "                            d[j] = i\n",
    "                            continue\n",
    "                chars2_length = [len(c) for c in chars2_sorted]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2_sorted):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "            \n",
    "            if parameters['char_mode'] == 'CNN':\n",
    "                d = {}\n",
    "                chars2_length = [len(c) for c in chars2]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "            targets = torch.LongTensor(tags)\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "            loss = neg_log_likelihood.data / len(data['words'])\n",
    "            losses.append(loss)\n",
    "            p_bar.set_postfix_str(f'Loss {loss}')\n",
    "            neg_log_likelihood.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # One epoch adjust lr\n",
    "        adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate))\n",
    "        \n",
    "       \n",
    "        print(f'Epoch {epoch} Validation starts')\n",
    "        valid_score = evaluate(model, val_data)\n",
    "        valid_scores.append(valid_score)\n",
    "        print(f'Current valid score {valid_score:.3f} | Best valid  score {best_valid:.3f} ')\n",
    "        if valid_score > best_valid:\n",
    "            print(f'Saving to {save_dir}')\n",
    "            torch.save(model.state_dict(), save_dir)\n",
    "            best_valid = valid_score\n",
    "            stop_count = 0\n",
    "        else:\n",
    "            print(f'No improvement for {stop_count} epoch')\n",
    "            stop_count += 1\n",
    "        if stop_count == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time and computation constraints, we only train our model for 20 epochs with early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14041 [00:00<?, ?it/s, Loss 2.6647584438323975]/tmp/ipykernel_76489/16323878.py:54: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
      "100%|██████████| 14041/14041 [10:06<00:00, 23.14it/s, Loss 9.15527380129788e-06]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [04:04<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.828 | Best valid  score 0.000 \n",
      "Saving to models/cnn-word.pt\n",
      "Epoch 2 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [10:11<00:00, 22.96it/s, Loss 9.482247696723789e-05]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [04:05<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.853 | Best valid  score 0.000 \n",
      "Saving to models/cnn-word.pt\n",
      "Epoch 3 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [10:10<00:00, 22.99it/s, Loss 8.803147647995502e-05]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [04:04<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.860 | Best valid  score 0.000 \n",
      "Saving to models/cnn-word.pt\n",
      "Epoch 4 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [10:12<00:00, 22.94it/s, Loss 0.003604888916015625]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [03:43<00:00, 14.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.873 | Best valid  score 0.000 \n",
      "Saving to models/cnn-word.pt\n",
      "Epoch 5 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [10:10<00:00, 23.01it/s, Loss 1.811981201171875e-05]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [04:04<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.870 | Best valid  score 0.000 \n",
      "Saving to models/cnn-word.pt\n",
      "Epoch 6 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 145/14041 [00:06<10:18, 22.47it/s, Loss 0.001172614865936339]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76489/2020660036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordCNNmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_76489/16323878.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, data)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mp_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss {loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/phy-env/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/phy-env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(20, WordCNNmodel, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is very slow (10 mins per epoch, 20 epochs will take more than 3 hours, so we interrupt the training). Since this is just an assignment which doesn't require comparing our model with the state-of-the-art. Later on, we only train 5 epochs for each model. As shown in the cell above, even we only train for 5 epochs, our model acheives decent performance (0.873 f1-score) on validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate its performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3453/3453 [03:37<00:00, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Word-level & CNN Char-level:  0.8163854552016602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "WordCNNmodel.load_state_dict(torch.load('models/cnn-word.pt'))\n",
    "WordCNNmodel = WordCNNmodel.cuda()\n",
    "test_score = evaluate(WordCNNmodel, test_data)\n",
    "print('CNN Word-level & CNN Char-level: ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question one (v)\n",
    "In the section, we compare the performance of our previous \"CNN-Char-CNN-Word\" model with \"LSTM-CHAR-CNN-Word\" model. To do so, we change the parameters['char_mode'] to 'lstm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In prvevious sections, the out channels for CNN-Char-Encoder is 25. For a fair comparison, we also set the __char_lstm_dim__=25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['char_mode'] = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharLSTMWordCNNmodel = OneCNN_WordEncoderModel(\n",
    "                   char_lstm_dim=25,\n",
    "                   vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf=parameters['crf'],\n",
    "                   char_mode='LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2483155"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(CharLSTMWordCNNmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14041 [00:00<?, ?it/s, Loss 1.5996241569519043]/tmp/ipykernel_78698/3021253189.py:53: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
      "100%|██████████| 14041/14041 [15:25<00:00, 15.17it/s, Loss 0.0014972686767578125]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [03:13<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.824 | Best valid  score 0.000 \n",
      "Saving to models/cnn-char-cnn-word.pt\n",
      "Epoch 2 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [15:24<00:00, 15.19it/s, Loss 0.008248466067016125]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [03:35<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.859 | Best valid  score 0.824 \n",
      "Saving to models/cnn-char-cnn-word.pt\n",
      "Epoch 3 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [15:23<00:00, 15.20it/s, Loss 0.002460055984556675]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [04:32<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.864 | Best valid  score 0.859 \n",
      "Saving to models/cnn-char-cnn-word.pt\n",
      "Epoch 4 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [15:32<00:00, 15.05it/s, Loss 0.008756637573242188]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [04:22<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.878 | Best valid  score 0.864 \n",
      "Saving to models/cnn-char-cnn-word.pt\n",
      "Epoch 5 training starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [15:22<00:00, 15.22it/s, Loss 3.24249267578125e-05]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [04:24<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valid score 0.879 | Best valid  score 0.878 \n",
      "Saving to models/cnn-char-cnn-word.pt\n"
     ]
    }
   ],
   "source": [
    "train(5, CharLSTMWordCNNmodel, train_data, save_dir='models/lstm-char-cnn-word.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3453/3453 [03:45<00:00, 15.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Word-level & LSTM Char-level:  0.8307967770814682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CharLSTMWordCNNmodel.load_state_dict(torch.load('models/lstm-char-cnn-word.pt'))\n",
    "CharLSTMWordCNNmodel = CharLSTMWordCNNmodel.cuda()\n",
    "test_score = evaluate(CharLSTMWordCNNmodel, test_data)\n",
    "print('CNN Word-level & LSTM Char-level: ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the resuts at the table below. Char-LSTM-Word-CNN achieves better f1-score with more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model              | Parameters | F1-Score |\n",
    "|--------------------|------------|----------|\n",
    "| Char-CNN-Word-CNN  | 2434655    | 0.8163   |\n",
    "| Char-LSTM-Word-CNN | 2483155    | __0.8307__   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question One (vi)\n",
    "In this section, we increase the number of CNN layer for word-level encoder.\n",
    "Let's try a 3-layer-CNN for word encoding. First of all let's switch back to the CNN-based character-level encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['char_mode'] = 'CNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerCNN_WordEncoderModel(OneCNN_WordEncoderModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Init word-level cnn\n",
    "        \n",
    "        cnns = [\n",
    "            nn.Conv1d(\n",
    "            in_channels=self.embedding_dim + self.out_channels,\n",
    "            out_channels=2*self.hidden_dim,\n",
    "            kernel_size=KERNAL_SIZE,\n",
    "            padding=1), \n",
    "            nn.Conv1d(\n",
    "            in_channels=self.embedding_dim + self.out_channels,\n",
    "            out_channels=2*self.hidden_dim,\n",
    "            kernel_size=KERNAL_SIZE,\n",
    "            padding=1), \n",
    "            nn.Conv1d(\n",
    "            in_channels=self.embedding_dim + self.out_channels,\n",
    "            out_channels=2*self.hidden_dim,\n",
    "            kernel_size=KERNAL_SIZE,\n",
    "            padding=1), \n",
    "            \n",
    "        ]\n",
    "        self.word_cnn = nn.Sequential(*cnns)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f715360d3d4ecb41d67c7d5157bdbc88a6ec901ac9ed57d2d8213181432edb4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
